# -*- coding: utf-8 -*-
"""Exercio_de_Classificação.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fg_k01JqwWg1VmoHimu3XBY42eqlxTHe

# Exercício de Classificação - Predição na qualidade do leite

Feature:
- pH
- Temprature
- Taste
- Odor
- Fat
- Turbidity
- Colour

Target:
- Grade


To-Dos do exercício:

- Transformar a variável target (Grade) de string para numérica (0,1,2). Lembrando que é um problema multiclasse (Uma única coluna de target) e não multi-label (Múltiplas colunas de target).
- Separação de Treino X Teste:
> Separa o conjunto de treino e teste com os dados resultante em 50% para cada.
- Normalização dos dados:
> Utilizando a função StandardScaler já fornecida, normaliza os dados de treino (Somente é usado o fit em cima dos dados de treino) e teste das colunas pH, Temperature e Colour.
- Treinar o modelo
- Validação do modelo:
> Analisar a acurácia do modelo com dados de teste
> Verificar outras métricas de performance do modelo



Dataset: https://www.kaggle.com/datasets/cpluzshrijayan/milkquality

# Importação das bibliotecas que irá utilizar
"""

# Importação de bibliotecas
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
from sklearn.model_selection import train_test_split

"""# Análise primária (Estatística/ Valores nulos/ Dados coerentes) e EDA"""

df = pd.read_csv('/content/milknew.csv')
df.head()

df.info()

df.columns = ['ph', 'temp', 'taste', 'odor', 'fat', 'turb', 'colour',
       'target']

df.describe(percentiles=[0.1,0.8,0.9,0.95,0.99])

sns.countplot(data=df,x='taste',hue='target')

sns.countplot(data=df,x='fat',hue='target')

sns.countplot(data=df,x='turb',hue='target')

sns.histplot(data=df,kde=True,x='ph',hue='target')

sns.histplot(data=df,kde=True,x='temp',hue='target')

sns.histplot(data=df,kde=True,x='colour',hue='target')

"""# Limpeza e tratamento dos dados

## IQR
"""

df_aux = df.copy()
for column in ['ph', 'temp','colour']:
  Q1 = df[column].quantile(0.25)
  Q3 = df[column].quantile(0.75)
  IQR = Q3 - Q1

  df_aux = df_aux[(df_aux[column] >= Q1 - 2.5*IQR) & (df_aux[column] <= Q3 + 2.5*IQR)]
  

df = df_aux
df

"""## Design de Experimentos Fatorial


"""

df['taste'] = df['taste'].map({1:1,0:-1})
df['odor'] = df['odor'].map({1:1,0:-1})
df['fat'] = df['fat'].map({1:1,0:-1})
df['turb'] = df['turb'].map({1:1,0:-1})

lista = ['taste','odor','fat','turb']
for i in range(len(lista)):
  if i == len(lista)-1:
    break
  else:
    for j in range(i+1,len(lista)):
      column_1 = lista[i]
      column_2 = lista[j]
      
      df[column_1+'_'+column_2] = df[column_1]*df[column_2]

df.head()

"""Transformar a variável target (Grade) de string para numérica (0,1,2)

"""

df['target'] = df['target'].map({'low':0,'medium':1,'high':2})

df.head()

"""Separação de Treino X Teste:
Separa o conjunto de treino e teste com os dados resultante em 50% para cada.
"""

from sklearn.model_selection import train_test_split

X_features = df.drop(columns=['target'])
y_target = df['target']

y_target

X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.5, random_state=7)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""Normalização dos dados:
Utilizando a função StandardScaler já fornecida, normaliza os dados de treino (Somente é usado o fit em cima dos dados de treino) e teste das colunas pH, Temperature e Colour.
"""

from sklearn.preprocessing import MinMaxScaler, StandardScaler

scaler = StandardScaler()
scaler.fit(X_train[['ph','temp','colour']])

X_train[['ph','temp','colour']] = scaler.transform(X_train[['ph','temp','colour']]) 
X_test[['ph','temp','colour']] = scaler.transform(X_test[['ph','temp','colour']])

X_train

"""Treinar o modelo

KNN
"""

from sklearn.metrics import accuracy_score, f1_score

# KNN
from sklearn.neighbors import KNeighborsClassifier

model_knn = KNeighborsClassifier()
model_knn.fit(X_train, y_train)

y_pred = model_knn.predict(X_test)

print("Acc Teste: ", accuracy_score(y_test, y_pred)*100)
print("F1 Teste: ", f1_score(y_test, y_pred, average='weighted')*100)

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier(n_estimators=50)
model_rf.fit(X_train, y_train)

y_pred = model_rf.predict(X_test)

print("Acc Teste: ", accuracy_score(y_test, y_pred)*100)
print("F1 Teste: ", f1_score(y_test, y_pred, average='weighted')*100)

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

clf = GaussianNB()

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

from sklearn.metrics import accuracy_score

print("Acc Teste: ", accuracy_score(y_test, y_pred)*100)
print("F1 Teste: ", f1_score(y_test, y_pred, average='weighted')*100)